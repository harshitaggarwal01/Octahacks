# -*- coding: utf-8 -*-
"""CNN,CNN+LSTM,LSTM_Wit_Word2Vec.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1C_ncUaTfh2Se6uBaNKsMWqbBcAVRQaLG
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# text preprocessing
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
import string
import re

# plots and metrics
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score, f1_score, confusion_matrix

# preparing input to our model
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical

# tensorflow keras layers
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense,Dropout,LSTM,Bidirectional,GRU,SpatialDropout1D,MaxPooling1D

num_classes = 5

# Number of dimensions for word embedding
embed_num_dims = 300

# Max input length (max number of words) 
max_seq_len = 500

class_names = ['joy', 'fear', 'anger', 'sadness', 'neutral']

data_train = pd.read_csv('data_train.csv', encoding='utf-8')
data_test = pd.read_csv('data_test.csv', encoding='utf-8')

data_train

data_test

X_train = data_train["Text"]
X_test = data_test["Text"]

y_train = data_train["Emotion"]
y_test = data_test["Emotion"]

#Merging the Datasets--> Train + Test  into 'data'
data = data_train.append(data_test, ignore_index=True)

data

#Unqiue Sentiments of Data Count:
data["Emotion"].value_counts()





"""### Pre Processing"""

PUNCT_TO_REMOVE = string.punctuation
def remove_punctuation(text):
    """custom function to remove the punctuation"""
    return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))

def clean_text(data):
    
    #Removing Punctuation Marks, unecessary symbols and everything:
    data = re.sub(r'\[[0-9]*\]',' ',str(data))
    data = re.sub(r'[^\w\s]','',data)
    data = re.sub(r'\s+',' ',data)
    data = data.lower()
    data = re.sub(r'\d',' ',data)
    data = re.sub(r'\s+',' ',data)
    # remove hashtags and @usernames
    data = re.sub(r"(#[\d\w\.]+)", '', data)
    data = re.sub(r"(@[\d\w\.]+)", '', data)
    
    # tekenization using nltk
    data = word_tokenize(data)
    
    stop_words = set(stopwords.words('english'))
    filtered_sentence = [w for w in data if not w in stop_words]
    data = ' '.join(filtered_sentence)
    
    return data

remove_punctuation(data.Text[92])

import nltk
nltk.download('punkt')
nltk.download('stopwords')

#Clean text Function will Return the tokens or words splitted from a unique sentence.
clean_text(data.Text[2])
clean_text(data.Text[4])
clean_text(data.Text[6])
clean_text(data.Text[9])

texts = [''.join(clean_text(text)) for text in data.Text]

texts_train = [''.join(clean_text(text)) for text in X_train]
texts_test = [''.join(clean_text(text)) for text in X_test]

texts

data["Text"][92]

texts_train[92]





# Using Tensorflow Keras Tokenizer to : Create a Internal Vocabulary -- Words to Integers ; 
#Arrange Sentences in Integers sequence formats

tokenizer = Tokenizer()
tokenizer.fit_on_texts(texts)

sequence_train = tokenizer.texts_to_sequences(texts_train)
sequence_test = tokenizer.texts_to_sequences(texts_test)

index_of_words = tokenizer.word_index

# vacab size is number of unique words + reserved 0 index for padding
vocab_size = len(index_of_words) + 1

print('Number of unique words: {}'.format(len(index_of_words)))



sequence_train,sequence_test





#Padding--> To make sure input sentence in our model is of same length

X_train_pad = pad_sequences(sequence_train, maxlen = max_seq_len )
X_test_pad = pad_sequences(sequence_test, maxlen = max_seq_len )

X_train_pad,X_test_pad

X_train_pad.shape,X_test_pad.shape

y_train

y_train,y_test

## Categorizing Labels:

encoding = {
    'joy': 0,
    'fear': 1,
    'anger': 2,
    'sadness': 3,
    'neutral': 4
}

# Integer labels
y_train = [encoding[x] for x in data_train.Emotion]
y_test = [encoding[x] for x in data_test.Emotion]

y_train,y_test

#Converting Categories to Array of Numbers For Model Classification
y_train = to_categorical(y_train)
y_test = to_categorical(y_test)

y_train

y_train.shape,y_test.shape

"""## Embedding Matrix and PreTrained Word Vectors:

"""

def create_embedding_matrix(filepath, word_index, embedding_dim):
    vocab_size = len(word_index) + 1  # Adding again 1 because of reserved 0 index
    embedding_matrix = np.zeros((vocab_size, embedding_dim))
    with open(filepath,encoding='utf-8') as f:
        for line in f:
            word, *vector = line.split()
            if word in word_index:
                idx = word_index[word] 
                embedding_matrix[idx] = np.array(
                    vector, dtype=np.float32)[:embedding_dim]
    return embedding_matrix

import urllib.request
import zipfile
import os

fname = 'embeddings/wiki-news-300d-1M.vec'

if not os.path.isfile(fname):
    print('Downloading word vectors...')
    urllib.request.urlretrieve('https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip',
                              'wiki-news-300d-1M.vec.zip')
    print('Unzipping...')
    with zipfile.ZipFile('wiki-news-300d-1M.vec.zip', 'r') as zip_ref:
        zip_ref.extractall('embeddings')
    print('done.')

embedd_matrix = create_embedding_matrix(fname, index_of_words, embed_num_dims)
embedd_matrix.shape

new_words = 0

for word in index_of_words:
    entry = embedd_matrix[index_of_words[word]]
    if all(v == 0 for v in entry):
        new_words = new_words + 1

print('Words found in wiki vocab: ' + str(len(index_of_words) - new_words))
print('New words found: ' + str(new_words))



"""### Model TryOuts:"""

# Embedding layer before the actaul BLSTM 
embedd_layer = Embedding(vocab_size,
                         embed_num_dims,
                         input_length = max_seq_len,
                         weights = [embedd_matrix],
                         trainable=False)



model = Sequential()
model.add(embedd_layer)
model.add(SpatialDropout1D(0.2))
model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))
model.add(Dense(num_classes, activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model.summary()
epochs = 12
batch_size = 256

history = model.fit(X_train_pad, y_train, epochs=epochs, batch_size=batch_size,validation_data=(X_test_pad,y_test),callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])

accr = model.evaluate(X_test_pad,y_test)
print('Test set\n  Loss: {:0.3f}\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))

model.save('LSTM_with_word2vec.h5')

model.save('LSTM_with_word2vec.h5')
model_file = drive.CreateFile({'title' : 'LSTM_with_word2vec.h5'}) 
model_file.SetContentFile('LSTM_with_word2vec.h5') 
model_file.Upload()

model1 = Sequential()
model1.add(embedd_layer)
model1.add(SpatialDropout1D(0.2))
model1.add(Conv1D(filters=256, kernel_size=3, padding='same', activation='relu'))
model1.add(MaxPooling1D(pool_size=2))
model1.add(SpatialDropout1D(0.2))
model1.add(LSTM(100))
#model.add(Dense(256, activation='relu'))

model1.add(Dense(num_classes, activation='softmax'))

model1.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])
model1.summary()

epochs = 18
batch_size = 256

history = model1.fit(X_train_pad, y_train, epochs=epochs, batch_size=batch_size,validation_data=(X_test_pad,y_test))

accr = model1.evaluate(X_test_pad,y_test)
print('Test set\n  Loss: {:0.3f}\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))

!pip install -U -q PyDrive
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive 
from google.colab import auth 
from oauth2client.client import GoogleCredentials

auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()  
drive = GoogleDrive(gauth)

model1.save('CNN_with_LSTM_with_word2vec.h5')
model1_file = drive.CreateFile({'title' : 'CNN_with_LSTM_with_word2vec.h5'}) 
model1_file.SetContentFile('CNN_with_LSTM_with_word2vec.h5') 
model1_file.Upload()





from google.colab import drive
drive.mount('/content/drive')



# Convolution
kernel_size = 3
filters = 256


model2 = Sequential()
model2.add(embedd_layer)
model2.add(Conv1D(filters, kernel_size, activation='relu'))
model2.add(GlobalMaxPooling1D())

model2.add(Dense(256, activation='relu'))
model2.add(Dense(num_classes, activation='softmax'))

model2.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])
model2.summary()

batch_size = 256
epochs = 18

hist = model2.fit(X_train_pad, y_train, 
                 batch_size=batch_size,
                 epochs=epochs,
                 validation_data=(X_test_pad,y_test))

accr = model2.evaluate(X_test_pad,y_test)
print('Test set\n  Loss: {:0.3f}\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))

model2.save('CNN_with_word2vec.h5')

model2.save('CNN_with_word2vec.h5')
model2_file = drive.CreateFile({'title' : 'CNN_with_word2vec.h5'}) 
model2_file.SetContentFile('CNN_with_word2vec.h5') 
model2_file.Upload()

# Accuracy plot
plt.plot(hist.history['accuracy'])
plt.plot(hist.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.show()

# Loss plot
plt.plot(hist.history['loss'])
plt.plot(hist.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.show()

"""## Predictions:"""

predictions = model.predict(X_test_pad)
predictions = np.argmax(predictions, axis=1)
predictions = [class_names[pred] for pred in predictions]
predictions

print("Accuracy: {:.2f}%".format(accuracy_score(data_test.Emotion, predictions) * 100))
print("\nF1 Score: {:.2f}".format(f1_score(data_test.Emotion, predictions, average='micro') * 100))



# creates a HDF5 file 'my_model.h5'
model.save('models/cnn_word21vec.h5')

from keras.models import load_model
predictors = load_model('models/cnn_word21vec.h5')

"""### Test Phase :"""

print('Message: {}\nPredicted: {}'.format(X_test[0], predictions[0]))

print('Message: {}\nPredicted: {}'.format(X_test[50], predictions[50]))

"""### Test Phase : With Manual Input:"""

import time

message = ['delivery was hour late and my pizza was cold!']

seq = tokenizer.texts_to_sequences(message)
padded = pad_sequences(seq, maxlen=max_seq_len)

start_time = time.time()
pred = model.predict(padded)

print('Message: ' + str(message))
print('predicted: {} ({:.2f} seconds)'.format(class_names[np.argmax(pred)], (time.time() - start_time)))

import time

message = ['anger']

seq = tokenizer.texts_to_sequences(message)
padded = pad_sequences(seq, maxlen=max_seq_len)

start_time = time.time()
pred = model.predict(padded)

print('Message: ' + str(message))
print('predicted: {} ({:.2f} seconds)'.format(class_names[np.argmax(pred)], (time.time() - start_time)))





num_classes = 5

max_seq_len = 500

class_names = ['joy', 'fear', 'anger', 'sadness', 'neutral']

message = ['i am happy from you!']

tokenizer = Tokenizer()
tokenizer.fit_on_texts(message)

seq = tokenizer.texts_to_sequences(message)
padded = pad_sequences(seq, maxlen=max_seq_len)


pred = predictor.predict(padded)







print('Message: ' + str(message))
print('predicted: {} '.format(class_names[np.argmax(pred)], ))

message = ['i am happy from you!']
tokenizer = Tokenizer()
tokenizer.fit_on_texts(message)
seq = tokenizer.texts_to_sequences(message)
padded = pad_sequences(seq, maxlen=max_seq_len)

predictions = predictor.predict(padded)
predictions = np.argmax(predictions, axis=1)
predictions = [class_names[pred] for pred in predictions]
predictions



